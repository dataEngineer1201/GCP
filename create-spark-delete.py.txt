import airflow
from datetime import datetime, timedelta
from airflow import DAG
from airflow.contrib.operators.dataproc_operator import DataprocClusterCreateOperator , DataProcPySparkOperator
from airflow.contrib.operators.dataproc_operator import DataprocClusterDeleteOperator



default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 8, 25),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'clucreate',
    default_args=default_args,
    description='Create a Dataproc cluster',
    schedule_interval=None,  # Set your desired schedule interval
    catchup=False,  # Only run the latest instance when catching up
)

cluster_name = 'mydp02'
project_id = 'my-new-project0123'
region = 'us-central1'  # Change to your desired region

create_cluster = DataprocClusterCreateOperator(
    task_id='create_dataproc_cluster',
    cluster_name=cluster_name,
    project_id=project_id,
    num_workers=2,
    region=region,
    dag=dag,
)

spark_job_task = DataProcPySparkOperator(
    task_id='run_spark_job',
    main='gs://sam01/mysp.py',  # Path to your Spark job script
    cluster_name=cluster_name,  # Existing cluster name
    region='us-central1',
    dag=dag,
)
delete_cluster = DataprocClusterDeleteOperator(
    task_id='delete_dataproc_cluster',
    cluster_name=cluster_name,
    project_id=project_id,
    region=region,
    trigger_rule='all_done',  # Delete the cluster when all tasks are done
    dag=dag,
)
create_cluster >> spark_job_task >> delete_cluster
